{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from src.models import CrossEntropyClassification\n",
    "from src.data import train_val_test_split, get_descriptor_and_labels\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, RichProgressBar\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimising NN parameters\n",
    "\n",
    "We use Bayesian optimisation implemented in Optuna to optimise the following NN parameters:\n",
    "- Number of hidden layers\n",
    "- Number of neurons per hidden layer\n",
    "- Learning rate\n",
    "- Weight decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the training, validation and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_structs, val_structs, test_structs = train_val_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1285, 20, 1245)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_structs), len(val_structs), len(test_structs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "numb_train_samples = 8_000\n",
    "\n",
    "train_x, train_y, label_mapping = get_descriptor_and_labels(train_structs, num_samples_per_type=numb_train_samples)\n",
    "val_x, val_y, _ = get_descriptor_and_labels(val_structs, num_samples_per_type=2_500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# standardize data\n",
    "scaler = preprocessing.StandardScaler().fit(train_x)\n",
    "scaled_train_x = torch.FloatTensor(scaler.transform(train_x))\n",
    "scaled_val_x = torch.FloatTensor(scaler.transform(val_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(scaled_train_x,train_y)\n",
    "val_dataset = TensorDataset(scaled_val_x,val_y)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=250, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=10000, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the optimisation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from src.data import predict_test_set_classes\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "output_size = train_y.shape[1] # number of classes; 3 in this case (HDA, LDA, MDA)\n",
    "input_size = 30 # number of features, i.e. the length of the Steinhardt descriptor\n",
    "\n",
    "def optimise_NN(trial: optuna.Trial):\n",
    "    # Optuna optimisation function for the NN\n",
    "    \n",
    "    # 1. Suggest the hyperparameters\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 5)\n",
    "    neurons_per_layer = trial.suggest_int(\"n_units_l0\", 8, 256, log=True)\n",
    "    hidden_units = [neurons_per_layer] * n_layers\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-8, 1e-1, log=True)\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-1, log=True)\n",
    "    \n",
    "\n",
    "    # 2. Create the model\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    model = CrossEntropyClassification(\n",
    "        input_size,\n",
    "        *hidden_units,\n",
    "        output_size,\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "    )\n",
    "\n",
    "    # 3. Train the model\n",
    "    trainer = Trainer(\n",
    "        accelerator=\"auto\",\n",
    "        max_epochs=200,\n",
    "        callbacks=[\n",
    "            RichProgressBar(),\n",
    "            EarlyStopping(monitor=\"validation_loss\", patience=10),\n",
    "        ],\n",
    "        logger=TensorBoardLogger(\"lightning_logs\"),\n",
    "    )\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "    \n",
    "    # 4. Load the best model\n",
    "    model.load_state_dict(torch.load(trainer.checkpoint_callback.best_model_path)['state_dict'])\n",
    "    \n",
    "    # 5. Evaluate the model on the validation set\n",
    "    pred_classes, val_classes, _ = predict_test_set_classes(val_structs,model=model, scaler=scaler)\n",
    "    \n",
    "    return balanced_accuracy_score(val_classes, pred_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Optimise the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-17 11:52:08,608] Using an existing study with name 'optimise_NN' instead of creating a new one.\n"
     ]
    }
   ],
   "source": [
    "study_name = \"optimise_NN\"  # Unique identifier of the study.\n",
    "storage_name = f\"sqlite:///{study_name}.db\"\n",
    "study = optuna.create_study(study_name=study_name, storage=storage_name, direction=\"maximize\",load_if_exists=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we perform 110 trials\n",
    "study.optimize(optimise_NN, n_trials=110)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = study.trials_dataframe(attrs=(\"number\", \"value\", \"params\"))\n",
    "df.sort_values(by=\"value\", ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>value</th>\n",
       "      <th>params_learning_rate</th>\n",
       "      <th>params_n_layers</th>\n",
       "      <th>params_n_units_l0</th>\n",
       "      <th>params_weight_decay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>82</td>\n",
       "      <td>0.857914</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>3</td>\n",
       "      <td>82</td>\n",
       "      <td>0.006364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>95</td>\n",
       "      <td>0.857698</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>3</td>\n",
       "      <td>73</td>\n",
       "      <td>0.007273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0.857695</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>3</td>\n",
       "      <td>128</td>\n",
       "      <td>0.005039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.857629</td>\n",
       "      <td>0.000924</td>\n",
       "      <td>2</td>\n",
       "      <td>117</td>\n",
       "      <td>0.003476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>0.857506</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>3</td>\n",
       "      <td>110</td>\n",
       "      <td>0.013906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>50</td>\n",
       "      <td>0.836967</td>\n",
       "      <td>0.001145</td>\n",
       "      <td>4</td>\n",
       "      <td>165</td>\n",
       "      <td>0.000130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>47</td>\n",
       "      <td>0.834233</td>\n",
       "      <td>0.000296</td>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>0.092648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.724030</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>3</td>\n",
       "      <td>110</td>\n",
       "      <td>0.097190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.001573</td>\n",
       "      <td>4</td>\n",
       "      <td>39</td>\n",
       "      <td>0.061781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>39</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000154</td>\n",
       "      <td>4</td>\n",
       "      <td>96</td>\n",
       "      <td>0.077242</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>110 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    number     value  params_learning_rate  params_n_layers  \\\n",
       "82      82  0.857914              0.000136                3   \n",
       "95      95  0.857698              0.000330                3   \n",
       "27      27  0.857695              0.000316                3   \n",
       "24      24  0.857629              0.000924                2   \n",
       "31      31  0.857506              0.000112                3   \n",
       "..     ...       ...                   ...              ...   \n",
       "50      50  0.836967              0.001145                4   \n",
       "47      47  0.834233              0.000296                2   \n",
       "0        0  0.724030              0.000017                3   \n",
       "9        9  0.333333              0.001573                4   \n",
       "39      39  0.333333              0.000154                4   \n",
       "\n",
       "    params_n_units_l0  params_weight_decay  \n",
       "82                 82             0.006364  \n",
       "95                 73             0.007273  \n",
       "27                128             0.005039  \n",
       "24                117             0.003476  \n",
       "31                110             0.013906  \n",
       "..                ...                  ...  \n",
       "50                165             0.000130  \n",
       "47                 35             0.092648  \n",
       "0                 110             0.097190  \n",
       "9                  39             0.061781  \n",
       "39                 96             0.077242  \n",
       "\n",
       "[110 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performance\n",
    "\n",
    "From the results, we observe that the model is very insensitive to parameter variation.\n",
    "We quantify this by evaluating the performance of some of the best and worst models.\n",
    "It is clear the bottom three models are much worse than the rest so we evaluated the 4th worst model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_best = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_best:\n",
    "    # Train the model with the optimised hyperparameters\n",
    "    optimised_NN_params = study.best_params\n",
    "    n_layers, neurons_per_layer, weight_decay, lr = optimised_NN_params.values()\n",
    "    hidden_layers = [neurons_per_layer] * n_layers\n",
    "else:\n",
    "    # Train the model with the worst hyperparameters in the top 97% of models\n",
    "    n_layers = df.iloc[106]['params_n_layers']\n",
    "    neurons_per_layer = df.iloc[106]['params_n_units_l0']\n",
    "    hidden_layers = [int(neurons_per_layer)] * int(n_layers)\n",
    "    weight_decay = df.iloc[106]['params_weight_decay']\n",
    "    lr = df.iloc[106]['params_learning_rate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[35, 35]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">   </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Name       </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Type       </span>┃<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\"> Params </span>┃\n",
       "┡━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 0 </span>│ activation │ ReLU       │      0 │\n",
       "│<span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 1 </span>│ layers     │ ModuleList │  2.5 K │\n",
       "└───┴────────────┴────────────┴────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━┓\n",
       "┃\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mName      \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mType      \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mParams\u001b[0m\u001b[1;35m \u001b[0m┃\n",
       "┡━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━┩\n",
       "│\u001b[2m \u001b[0m\u001b[2m0\u001b[0m\u001b[2m \u001b[0m│ activation │ ReLU       │      0 │\n",
       "│\u001b[2m \u001b[0m\u001b[2m1\u001b[0m\u001b[2m \u001b[0m│ layers     │ ModuleList │  2.5 K │\n",
       "└───┴────────────┴────────────┴────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Trainable params</span>: 2.5 K                                                                                            \n",
       "<span style=\"font-weight: bold\">Non-trainable params</span>: 0                                                                                            \n",
       "<span style=\"font-weight: bold\">Total params</span>: 2.5 K                                                                                                \n",
       "<span style=\"font-weight: bold\">Total estimated model params size (MB)</span>: 0                                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mTrainable params\u001b[0m: 2.5 K                                                                                            \n",
       "\u001b[1mNon-trainable params\u001b[0m: 0                                                                                            \n",
       "\u001b[1mTotal params\u001b[0m: 2.5 K                                                                                                \n",
       "\u001b[1mTotal estimated model params size (MB)\u001b[0m: 0                                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3757147fb4324032965eb4110b2a51c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/zoefaurebeaulieu/miniconda3/envs/steinhardt/lib/python3.10/site-packages/pytorch_lightning/trainer/connector\n",
       "s/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may \n",
       "be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus \n",
       "on this machine) in the `DataLoader` init to improve performance.\n",
       "  rank_zero_warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/zoefaurebeaulieu/miniconda3/envs/steinhardt/lib/python3.10/site-packages/pytorch_lightning/trainer/connector\n",
       "s/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may \n",
       "be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus \n",
       "on this machine) in the `DataLoader` init to improve performance.\n",
       "  rank_zero_warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/zoefaurebeaulieu/miniconda3/envs/steinhardt/lib/python3.10/site-packages/pytorch_lightning/trainer/connector\n",
       "s/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which \n",
       "may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of \n",
       "cpus on this machine) in the `DataLoader` init to improve performance.\n",
       "  rank_zero_warn(\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/zoefaurebeaulieu/miniconda3/envs/steinhardt/lib/python3.10/site-packages/pytorch_lightning/trainer/connector\n",
       "s/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which \n",
       "may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of \n",
       "cpus on this machine) in the `DataLoader` init to improve performance.\n",
       "  rank_zero_warn(\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_size = scaled_train_x.shape[1]\n",
    "output_size = train_y.shape[1] \n",
    "\n",
    "torch.manual_seed(42)\n",
    "neural_net = CrossEntropyClassification(\n",
    "    input_size,\n",
    "    *hidden_layers,\n",
    "    output_size,\n",
    "    learning_rate=lr,\n",
    "    weight_decay=weight_decay,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "        accelerator=\"auto\",\n",
    "        max_epochs=200,\n",
    "        callbacks=[\n",
    "            RichProgressBar(),\n",
    "            EarlyStopping(monitor=\"validation_loss\", patience=10),\n",
    "        ],\n",
    "    )\n",
    "trainer.fit(neural_net, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Accuracy (%): 82.3\n"
     ]
    }
   ],
   "source": [
    "from src.data import predict_test_set_classes\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "pred_classes, test_classes, confidences = predict_test_set_classes(test_structs,model=neural_net, scaler=scaler)\n",
    "\n",
    "test_av_accuracy = balanced_accuracy_score(test_classes, pred_classes)\n",
    "print(f\"Balanced Accuracy (%): {test_av_accuracy*100:.1f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tet_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
